{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "from textattack.augmentation import WordNetAugmenter\n",
        "from typing import List, Dict\n",
        "\n",
        "# Configuration\n",
        "MODEL_CONFIGS = {\n",
        "    \"deberta-v3\": {\n",
        "        \"name\": \"microsoft/deberta-v3-base\",\n",
        "        \"max_length\": 256,\n",
        "        \"batch_size\": 4\n",
        "    }\n",
        "}\n",
        "\n",
        "class SpanDataset(Dataset):\n",
        "    def __init__(self, encodings, labels, offset_mappings):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.offset_mappings = offset_mappings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.encodings['input_ids'][idx],\n",
        "            'attention_mask': self.encodings['attention_mask'][idx],\n",
        "            'labels': self.labels[idx],\n",
        "            'offset_mapping': self.offset_mappings[idx]\n",
        "        }\n",
        "\n",
        "class TokenFocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2, class_weights=None):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = F.binary_cross_entropy_with_logits(\n",
        "            inputs, targets, reduction='none', weight=self.class_weights\n",
        "        )\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        return (self.alpha * (1 - pt)**self.gamma * bce_loss).mean()\n",
        "\n",
        "# Initialize text augmenter\n",
        "augmenter = WordNetAugmenter(pct_words_to_swap=0.1, transformations_per_example=2)\n",
        "\n",
        "# Modified data loading with augmentation\n",
        "def load_data(file_path: str, augment: bool = False) -> list:\n",
        "    with open(data_dir + file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if not augment:\n",
        "        return data\n",
        "\n",
        "    augmented_data = []\n",
        "    for item in data:\n",
        "        # Original item\n",
        "        augmented_data.append(item)\n",
        "\n",
        "        # Augmented versions\n",
        "        try:\n",
        "            augmented_text = augmenter.augment(item[\"text\"])\n",
        "            for text in augmented_text:\n",
        "                new_item = item.copy()\n",
        "                new_item[\"text\"] = text\n",
        "                augmented_data.append(new_item)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return augmented_data\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"/content/drive/My Drive/SEMEVAL/data/\"\n",
        "\n",
        "\n",
        "def convert_spans_to_token_labels(text: str, spans: List[dict], offset_mapping, max_length: int, label_map: dict):\n",
        "    labels = np.zeros((max_length, len(label_map)), dtype=int)\n",
        "\n",
        "    for span in spans:\n",
        "        technique = span[\"technique\"]\n",
        "        if technique not in label_map:\n",
        "            continue\n",
        "\n",
        "        class_idx = label_map[technique]\n",
        "        span_start = span[\"start\"]\n",
        "        span_end = span[\"end\"]\n",
        "\n",
        "        # Find all overlapping tokens\n",
        "        overlapping_tokens = []\n",
        "        for token_idx, (start, end) in enumerate(offset_mapping):\n",
        "            if token_idx >= max_length:\n",
        "                break\n",
        "            if max(start, span_start) < min(end, span_end):\n",
        "                overlapping_tokens.append(token_idx)\n",
        "\n",
        "        # Add context window (2 tokens before/after)\n",
        "        context_tokens = set()\n",
        "        for tok in overlapping_tokens:\n",
        "            context_tokens.update(range(max(0, tok-2), min(len(offset_mapping)-1, tok+3)))\n",
        "\n",
        "        for tok in context_tokens:\n",
        "            if tok < max_length:\n",
        "                labels[tok, class_idx] = 1\n",
        "\n",
        "    return labels\n",
        "\n",
        "def optimize_thresholds(probs, labels, label_map):\n",
        "    best_thresholds = {}\n",
        "    for class_idx in range(len(label_map)):\n",
        "        y_true = labels[:, :, class_idx].flatten()\n",
        "        y_probs = probs[:, :, class_idx].flatten()\n",
        "\n",
        "        thresholds = np.linspace(0.1, 0.9, 20)\n",
        "        best_f1 = 0\n",
        "        best_thresh = 0.4\n",
        "\n",
        "        for thresh in thresholds:\n",
        "            y_pred = (y_probs > thresh).astype(int)\n",
        "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_thresh = thresh\n",
        "\n",
        "        best_thresholds[class_idx] = best_thresh\n",
        "\n",
        "    return best_thresholds\n",
        "\n",
        "def prepare_datasets(config: dict, tokenizer, data):\n",
        "    processed = {'input_ids': [], 'attention_mask': [], 'labels': [], 'offset_mappings': []}\n",
        "    max_length = config[\"max_length\"]\n",
        "\n",
        "    for item in data:\n",
        "        # Tokenize with padding and truncation first\n",
        "        encoding = tokenizer(\n",
        "            item[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_offsets_mapping=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Convert spans using the same tokenization's offset mapping\n",
        "        labels = convert_spans_to_token_labels(\n",
        "            item[\"text\"],\n",
        "            item.get(\"labels\", []),\n",
        "            encoding[\"offset_mapping\"][0].tolist(),\n",
        "            max_length,\n",
        "            label_map\n",
        "        )\n",
        "\n",
        "        processed['input_ids'].append(encoding[\"input_ids\"][0])\n",
        "        processed['attention_mask'].append(encoding[\"attention_mask\"][0])\n",
        "        processed['labels'].append(torch.FloatTensor(labels))\n",
        "        processed['offset_mappings'].append(encoding[\"offset_mapping\"][0])\n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.stack(processed['input_ids']),\n",
        "        'attention_mask': torch.stack(processed['attention_mask']),\n",
        "        'labels': torch.stack(processed['labels']),\n",
        "        'offset_mappings': processed['offset_mappings']\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_metrics(preds, labels, offset_mappings, label_map, thresholds=None):\n",
        "    if thresholds is None:\n",
        "        thresholds = optimize_thresholds(preds, labels, label_map)\n",
        "\n",
        "    # Apply thresholds to get binary predictions\n",
        "    binarized_preds = []\n",
        "    for batch_probs in preds:\n",
        "        batch_preds = np.zeros_like(batch_probs)\n",
        "        for class_idx, thresh in thresholds.items():\n",
        "            batch_preds[:, :, class_idx] = (batch_probs[:, :, class_idx] > thresh).astype(int)\n",
        "        binarized_preds.append(batch_preds)\n",
        "\n",
        "    # Flatten and mask tokens for token-level micro F1\n",
        "    flat_labels = []\n",
        "    flat_preds = []\n",
        "    for batch_labels, batch_preds, batch_offsets in zip(labels, binarized_preds, offset_mappings):\n",
        "        for sample_labels, sample_preds, sample_offsets in zip(batch_labels, batch_preds, batch_offsets):\n",
        "            mask = sample_offsets[:, 0].astype(bool)  # Ignore padding tokens\n",
        "            flat_labels.append(sample_labels[mask])\n",
        "            flat_preds.append(sample_preds[mask])\n",
        "    flat_labels = np.concatenate(flat_labels)\n",
        "    flat_preds = np.concatenate(flat_preds)\n",
        "\n",
        "    # Token-level micro F1\n",
        "    micro_f1 = f1_score(flat_labels, flat_preds, average='micro', zero_division=0)\n",
        "\n",
        "    # Span-level F1 using helper function\n",
        "    span_f1 = calculate_span_f1(binarized_preds, labels, offset_mappings, label_map)\n",
        "\n",
        "    return {\n",
        "        \"token_micro_f1\": micro_f1,\n",
        "        \"span_f1\": span_f1,\n",
        "        \"thresholds\": thresholds\n",
        "    }\n",
        "\n",
        "\n",
        "def train_model(model_config: dict, train_data, dev_data, label_map):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_config[\"name\"], add_prefix_space=True)\n",
        "\n",
        "    # Prepare datasets with augmented training data\n",
        "    train_processed = prepare_datasets(model_config, tokenizer,\n",
        "                                      load_data(\"training_set_task2.txt\", augment=True))\n",
        "    dev_processed = prepare_datasets(model_config, tokenizer, dev_data)\n",
        "\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_config[\"name\"],\n",
        "        num_labels=len(label_map),\n",
        "        problem_type=\"multi_label_classification\"\n",
        "    ).to(device)\n",
        "\n",
        "    train_dataset = SpanDataset(\n",
        "        {'input_ids': train_processed['input_ids'], 'attention_mask': train_processed['attention_mask']},\n",
        "        train_processed['labels'],\n",
        "        train_processed['offset_mappings']\n",
        "    )\n",
        "    dev_dataset = SpanDataset(\n",
        "        {'input_ids': dev_processed['input_ids'], 'attention_mask': dev_processed['attention_mask']},\n",
        "        dev_processed['labels'],\n",
        "        dev_processed['offset_mappings']\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=model_config[\"batch_size\"], shuffle=True)\n",
        "    dev_loader = DataLoader(dev_dataset, batch_size=model_config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "    # Class weighting\n",
        "    class_counts = torch.zeros(len(label_map))\n",
        "    for batch in train_loader:\n",
        "        class_counts += batch['labels'].sum(dim=[0,1])\n",
        "    class_weights = (1 / (class_counts + 1e-6)).to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    criterion = TokenFocalLoss(alpha=0.25, gamma=2, class_weights=class_weights)\n",
        "\n",
        "    best_f1 = 0\n",
        "    best_thresholds = None\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Only pass input tensors to model\n",
        "            inputs = {\n",
        "                'input_ids': batch['input_ids'].to(device),\n",
        "                'attention_mask': batch['attention_mask'].to(device)\n",
        "            }\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # Get active tokens (shape: [batch_size, seq_len])\n",
        "            active_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            # Reshape and mask\n",
        "            logits = outputs.logits.view(-1, len(label_map))\n",
        "            labels = labels.view(-1, len(label_map))\n",
        "            active_mask = active_mask.view(-1)\n",
        "\n",
        "            # Apply mask\n",
        "            active_logits = logits[active_mask.bool()]\n",
        "            active_labels = labels[active_mask.bool()]\n",
        "\n",
        "            loss = criterion(active_logits, active_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        all_probs, all_labels, all_offsets = [], [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in dev_loader:\n",
        "                inputs = {\n",
        "                    'input_ids': batch['input_ids'].to(device),\n",
        "                    'attention_mask': batch['attention_mask'].to(device)\n",
        "                }\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                probs = torch.sigmoid(outputs.logits).cpu().numpy()\n",
        "\n",
        "                all_probs.append(probs)\n",
        "                all_labels.append(batch['labels'].numpy())\n",
        "                all_offsets.append(batch['offset_mapping'].numpy())\n",
        "\n",
        "        # Optimize thresholds on first epoch and every 5 epochs\n",
        "        if epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "            best_thresholds = optimize_thresholds(\n",
        "                np.concatenate(all_probs),\n",
        "                np.concatenate(all_labels),\n",
        "                label_map\n",
        "            )\n",
        "\n",
        "        # Apply best thresholds\n",
        "        all_preds = []\n",
        "        for probs in all_probs:\n",
        "            batch_preds = np.zeros_like(probs)\n",
        "            for class_idx, thresh in best_thresholds.items():\n",
        "                batch_preds[:, :, class_idx] = (probs[:, :, class_idx] > thresh).astype(int)\n",
        "            all_preds.append(batch_preds)\n",
        "\n",
        "       # Initialize metrics with default values\n",
        "        metrics = compute_metrics(\n",
        "        np.concatenate(all_probs),\n",
        "        np.concatenate(all_labels),\n",
        "        np.concatenate(all_offsets, axis=0),\n",
        "        label_map,\n",
        "        best_thresholds\n",
        "    )\n",
        "\n",
        "    micro_f1 = metrics[\"token_micro_f1\"]\n",
        "    span_f1 = metrics[\"span_f1\"]\n",
        "    best_thresholds = metrics[\"thresholds\"]\n",
        "\n",
        "    if span_f1 > best_f1:\n",
        "        best_f1 = span_f1\n",
        "        model_name = model_config['name'].replace('/', '_')\n",
        "        save_path = f\"best_{model_name}.pth\"\n",
        "        Path(save_path).parent.mkdir(exist_ok=True)\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Saved best model to {save_path}\")\n",
        "\n",
        "    return best_f1\n",
        "\n",
        "def calculate_span_f1(preds, labels, offset_mappings, label_map):\n",
        "    id_to_label = {v: k for k, v in label_map.items()}\n",
        "    true_spans = defaultdict(set)\n",
        "    pred_spans = defaultdict(set)\n",
        "\n",
        "    # Iterate through each batch\n",
        "    for batch_idx in range(len(preds)):\n",
        "        batch_preds = preds[batch_idx]\n",
        "        batch_labels = labels[batch_idx]\n",
        "        batch_offsets = offset_mappings[batch_idx]\n",
        "\n",
        "        # Iterate through each sample in the batch\n",
        "        for sample_idx in range(len(batch_preds)):\n",
        "            pred = batch_preds[sample_idx]\n",
        "            label = batch_labels[sample_idx]\n",
        "            offsets = batch_offsets[sample_idx]\n",
        "\n",
        "            for class_idx in label_map.values():\n",
        "                # True spans\n",
        "                current_span = []\n",
        "                for pos in range(len(label)):\n",
        "                    # Access scalar value with [pos, class_idx]\n",
        "                    if label[pos, class_idx] == 1:\n",
        "                        current_span.append(pos)\n",
        "                    elif current_span:\n",
        "                        try:\n",
        "                            start = offsets[current_span[0]][0].item()\n",
        "                            end = offsets[current_span[-1]][1].item()\n",
        "                            true_spans[class_idx].add((batch_idx, sample_idx, start, end))\n",
        "                        except:\n",
        "                            pass\n",
        "                        current_span = []\n",
        "                if current_span:  # Handle last span\n",
        "                    try:\n",
        "                        start = offsets[current_span[0]][0].item()\n",
        "                        end = offsets[current_span[-1]][1].item()\n",
        "                        true_spans[class_idx].add((batch_idx, sample_idx, start, end))\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                # Predicted spans\n",
        "                current_span = []\n",
        "                for pos in range(len(pred)):\n",
        "                    if pred[pos, class_idx] == 1:\n",
        "                        current_span.append(pos)\n",
        "                    elif current_span:\n",
        "                        try:\n",
        "                            start = offsets[current_span[0]][0].item()\n",
        "                            end = offsets[current_span[-1]][1].item()\n",
        "                            pred_spans[class_idx].add((batch_idx, sample_idx, start, end))\n",
        "                        except:\n",
        "                            pass\n",
        "                        current_span = []\n",
        "                if current_span:  # Handle last span\n",
        "                    try:\n",
        "                        start = offsets[current_span[0]][0].item()\n",
        "                        end = offsets[current_span[-1]][1].item()\n",
        "                        pred_spans[class_idx].add((batch_idx, sample_idx, start, end))\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "    # Calculate F1 per class\n",
        "    f1_scores = []\n",
        "    for class_idx in label_map.values():\n",
        "        tp = len(true_spans[class_idx] & pred_spans[class_idx])\n",
        "        fp = len(pred_spans[class_idx] - true_spans[class_idx])\n",
        "        fn = len(true_spans[class_idx] - pred_spans[class_idx])\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    return np.mean(f1_scores)\n",
        "# Execution\n",
        "NUM_EPOCHS = 10\n",
        "MODEL_TO_TRAIN = \"deberta-v3\"\n",
        "\n",
        "train_data = load_data(\"training_set_task2.txt\")\n",
        "dev_data = load_data(\"dev_set_task2.txt\")\n",
        "\n",
        "all_techniques = sorted({\n",
        "    label[\"technique\"] for item in train_data + dev_data\n",
        "    for label in item.get(\"labels\", [])\n",
        "})\n",
        "label_map = {tech: idx for idx, tech in enumerate(all_techniques)}\n",
        "\n",
        "print(f\"Training {MODEL_TO_TRAIN}...\")\n",
        "best_f1 = train_model(MODEL_CONFIGS[MODEL_TO_TRAIN], train_data, dev_data, label_map)\n",
        "print(f\"\\nBest Span F1: {best_f1:.4f}\")"
      ],
      "metadata": {
        "id": "88-LcOVk58Gc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}