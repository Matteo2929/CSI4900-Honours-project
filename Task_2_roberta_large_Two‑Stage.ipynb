{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============ 0. Imports and Setup ============\n",
        "import json, os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from tabulate import tabulate\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "import torchcrf\n",
        "from inspect import signature\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# ============ 1. Mount Drive and Load Data ============\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/My Drive/SEMEVAL/data\")\n",
        "train_file = DATA_DIR / \"training_set_task2.txt\"\n",
        "dev_file   = DATA_DIR / \"dev_set_task2.txt\"\n",
        "\n",
        "def load_jsonl(path):\n",
        "    return json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "train_data = load_jsonl(train_file)\n",
        "dev_data   = load_jsonl(dev_file)\n",
        "\n",
        "# ============ 2. Build Label Map ============\n",
        "all_techniques = sorted({\n",
        "    lbl[\"technique\"].strip()\n",
        "    for item in train_data + dev_data\n",
        "    for lbl in item.get(\"labels\", [])\n",
        "})\n",
        "label2id = {tech: i for i, tech in enumerate(all_techniques)}\n",
        "id2label = {i: tech for tech, i in label2id.items()}\n",
        "\n",
        "# BIO tagging labels\n",
        "bio_labels = [\"O\"] + [prefix + \"-\" + tech for tech in all_techniques for prefix in [\"B\", \"I\"]]\n",
        "bio_label2id = {lbl: i for i, lbl in enumerate(bio_labels)}\n",
        "bio_id2label = {i: lbl for lbl, i in bio_label2id.items()}\n",
        "\n",
        "# ============ 3. Tokenizer ============\n",
        "MODEL_NAME = \"roberta-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "# ============ 4. Stage 1 - Span Detection with BIO-CRF ============\n",
        "class SpanDetectorModel(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModelForTokenClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=num_labels,\n",
        "            problem_type=\"single_label_classification\"\n",
        "        )\n",
        "        self.crf = torchcrf.CRF(num_labels, batch_first=True)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        logits = self.backbone(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        if labels is not None:\n",
        "            loss = -self.crf(logits, labels, mask=attention_mask.bool(), reduction='mean')\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "        else:\n",
        "            pred = self.crf.decode(logits, mask=attention_mask.bool())\n",
        "            return {\"logits\": pred}\n",
        "\n",
        "span_detector = SpanDetectorModel(MODEL_NAME, len(bio_labels))\n",
        "\n",
        "def tokenize_and_bio(example):\n",
        "    tokens = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "    offsets = tokens.pop(\"offset_mapping\")\n",
        "    labels = [\"O\"] * len(offsets)\n",
        "\n",
        "    for span in example.get(\"labels\", []):\n",
        "        start_char, end_char = span[\"start\"], span[\"end\"]\n",
        "        tech = span[\"technique\"].strip()\n",
        "        first = True\n",
        "        for i, (s, e) in enumerate(offsets):\n",
        "            if s >= end_char or e <= start_char:\n",
        "                continue\n",
        "            labels[i] = (\"B-\" if first else \"I-\") + tech\n",
        "            first = False\n",
        "\n",
        "    tokens[\"labels\"] = [bio_label2id[lbl] for lbl in labels]\n",
        "    return tokens\n",
        "\n",
        "raw_datasets = DatasetDict({\n",
        "    \"train\": Dataset.from_list(train_data),\n",
        "    \"dev\":   Dataset.from_list(dev_data)\n",
        "})\n",
        "bio_tokenized = raw_datasets.map(tokenize_and_bio, batched=False)\n",
        "\n",
        "# ============ 5. Stage 2 - Span Classification ============\n",
        "span_classifier = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(label2id)\n",
        ")\n",
        "\n",
        "def extract_spans_from_bio(tokens, preds):\n",
        "    spans = []\n",
        "    current = None\n",
        "    for i, tag_idx in enumerate(preds):\n",
        "        tag = bio_id2label[tag_idx]\n",
        "        if tag == \"O\":\n",
        "            if current:\n",
        "                spans.append(current)\n",
        "                current = None\n",
        "        else:\n",
        "            prefix, tech = tag.split(\"-\", 1)\n",
        "            if prefix == \"B\" or (current and current[\"technique\"] != tech):\n",
        "                if current:\n",
        "                    spans.append(current)\n",
        "                current = {\"technique\": tech, \"start\": tokens[\"offset_mapping\"][i][0], \"end\": tokens[\"offset_mapping\"][i][1]}\n",
        "            else:  # I- continuation\n",
        "                current[\"end\"] = tokens[\"offset_mapping\"][i][1]\n",
        "    if current:\n",
        "        spans.append(current)\n",
        "    return spans\n",
        "\n",
        "# ============ 6. Class Weights for Imbalanced Classification ============\n",
        "label_counts = Counter()\n",
        "total_tokens = 0\n",
        "for ex in bio_tokenized[\"train\"]:\n",
        "    for lbl in ex[\"labels\"]:\n",
        "        if lbl != 0:\n",
        "            label_counts[lbl] += 1\n",
        "        total_tokens += 1\n",
        "\n",
        "class_weights = []\n",
        "for i in range(len(bio_labels)):\n",
        "    count = label_counts.get(i, 1)\n",
        "    weight = total_tokens / (len(bio_labels) * count)\n",
        "    class_weights.append(weight)\n",
        "\n",
        "class_weights_tensor = torch.tensor(class_weights)\n",
        "\n",
        "# ============ 7. Official Partial-Overlap Scorer ============\n",
        "def compute_PRF(gold_spans, pred_spans, label):\n",
        "    S = [s for s in pred_spans if s[\"technique\"] == label]\n",
        "    T = [t for t in gold_spans if t[\"technique\"] == label]\n",
        "    if not S or not T:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    def C(s,t):\n",
        "        if s[\"technique\"] != t[\"technique\"]: return 0\n",
        "        inter = max(0, min(s[\"end\"], t[\"end\"]) - max(s[\"start\"], t[\"start\"]))\n",
        "        return inter / (s[\"end\"] - s[\"start\"])\n",
        "    P = sum(max(C(s, t) for t in T) for s in S) / len(S)\n",
        "    R = sum(max(C(s, t) for s in S) for t in T) / len(T)\n",
        "    F1 = 2*P*R/(P+R) if (P+R)>0 else 0.0\n",
        "    return P, R, F1\n",
        "\n",
        "\n",
        "def evaluate_spans(examples, predictions, thresholds):\n",
        "    # examples: list of dicts with \"text\",\"labels\"\n",
        "    # predictions: raw logits [batch, seq_len, num_labels]\n",
        "    preds = torch.sigmoid(torch.tensor(predictions)).numpy()\n",
        "    all_gold = []\n",
        "    all_pred = []\n",
        "    for ex,logits in zip(examples, preds):\n",
        "        # reconstruct spans\n",
        "        # get offsets\n",
        "        enc = tokenizer(ex[\"text\"], truncation=True, padding=\"max_length\",\n",
        "                        max_length=256, return_offsets_mapping=True)\n",
        "        offsets = enc[\"offset_mapping\"]\n",
        "        gold = ex[\"labels\"]\n",
        "        pred = []\n",
        "        for lbl_idx, thresh in thresholds.items():\n",
        "            for i, (s, e) in enumerate(offsets):\n",
        "                if s < e and logits[i, lbl_idx] > thresh:\n",
        "                    # Use BIO labels\n",
        "                    bio_label = bio_id2label[lbl_idx]\n",
        "                    if bio_label == \"O\":\n",
        "                        continue  # Ignore 'O' tags\n",
        "                    prefix, technique = bio_label.split(\"-\", 1)\n",
        "                    pred.append({\"technique\": technique, \"start\": s, \"end\": e})\n",
        "    # compute per-label and overall micro\n",
        "    rows = []\n",
        "    micro_tp=micro_fp=micro_fn=0\n",
        "    for tech in all_techniques:\n",
        "        P_list,R_list,F1_list=[],[],[]\n",
        "        for gold,pred in zip(all_gold, all_pred):\n",
        "            p,r,f = compute_PRF(gold,pred,tech)\n",
        "            P_list.append(p); R_list.append(r); F1_list.append(f)\n",
        "            # for micro: count as if each example a unit\n",
        "            micro_tp += p\n",
        "            micro_fp += (1-p)\n",
        "            micro_fn += (1-r)\n",
        "        rows.append([tech,\n",
        "                     np.mean(P_list),\n",
        "                     np.mean(R_list),\n",
        "                     np.mean(F1_list)])\n",
        "    microP = micro_tp/(micro_tp+micro_fp) if micro_tp+micro_fp>0 else 0\n",
        "    microR = micro_tp/(micro_tp+micro_fn) if micro_tp+micro_fn>0 else 0\n",
        "    microF = 2*microP*microR/(microP+microR) if microP+microR>0 else 0\n",
        "    return rows, (microP,microR,microF)\n",
        "# ============ 8. Trainer ============\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    # Threshold optimization on dev batch\n",
        "    thresholds = {}\n",
        "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # One-hot encode labels to match [batch_size, seq_len, num_labels]\n",
        "    if labels.ndim == 2:  # (batch_size, seq_len)\n",
        "        one_hot_labels = np.zeros_like(probs)\n",
        "        for i in range(probs.shape[2]):\n",
        "            one_hot_labels[:, :, i] = (labels == i).astype(int)\n",
        "    else:\n",
        "        one_hot_labels = labels\n",
        "\n",
        "    for i in range(probs.shape[2]):\n",
        "        best_f, best_t = 0, 0.5\n",
        "        for t in np.arange(0.1, 0.9, 0.05):\n",
        "            pr = (probs[:, :, i] > t).astype(int)\n",
        "            f = f1_score(one_hot_labels[:, :, i].flatten(), pr.flatten(), zero_division=0)\n",
        "            if f > best_f:\n",
        "                best_f, best_t = f, t\n",
        "        thresholds[i] = best_t\n",
        "\n",
        "    # Evaluate using thresholds\n",
        "    rows, micro = evaluate_spans(\n",
        "        raw_datasets[\"dev\"].select(range(len(probs))),\n",
        "        logits,\n",
        "        thresholds\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Per-label results ===\")\n",
        "    print(tabulate(rows, headers=[\"Technique\", \"P\", \"R\", \"F1\"], floatfmt=\".3f\"))\n",
        "    print(f\"\\n=== Micro Overall ===  P={micro[0]:.3f}  R={micro[1]:.3f}  F1={micro[2]:.3f}\\n\")\n",
        "\n",
        "    return {\"micro_f1\": micro[2]}\n",
        "\n",
        "\n",
        "class BIOCRFTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs, labels=labels)\n",
        "        return (outputs[\"loss\"], outputs) if return_outputs else outputs[\"loss\"]\n",
        "\n",
        "bio_training_args = TrainingArguments(\n",
        "    output_dir=\"./bio_crf_results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "bio_trainer = BIOCRFTrainer(\n",
        "    model=span_detector,\n",
        "    args=bio_training_args,\n",
        "    train_dataset=bio_tokenized[\"train\"],\n",
        "    eval_dataset=bio_tokenized[\"dev\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# ============ 9. Train BIO-CRF ============\n",
        "bio_trainer.train()"
      ],
      "metadata": {
        "id": "88-LcOVk58Gc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}