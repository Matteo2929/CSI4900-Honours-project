{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "R1f1E7KP3bAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AdamW\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import os\n",
        "from typing import Dict\n",
        "\n",
        "# Configuration for models\n",
        "MODEL_CONFIGS = {\n",
        "    \"mistral-7b\": {\n",
        "        \"name\": \"mistralai/Mistral-7B-v0.1\",\n",
        "        \"quantize\": True,\n",
        "        \"max_length\": 1024,\n",
        "        \"batch_size\": 2\n",
        "    }\n",
        "}\n",
        "\n",
        "class MistralForMultiLabelClassification(nn.Module):\n",
        "    def __init__(self, model_name, num_labels, quantize_config=None):\n",
        "        super().__init__()\n",
        "        # Load base model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(  # FIXED THIS LINE\n",
        "            model_name,\n",
        "            quantization_config=quantize_config,\n",
        "            device_map=\"auto\",\n",
        "            token=os.environ.get(\"HF_TOKEN\")\n",
        "        )  # ADDED MISSING PARENTHESIS HERE\n",
        "\n",
        "        # Get device from base model\n",
        "        self.device = next(self.model.parameters()).device\n",
        "\n",
        "        # Add classification head on the same device\n",
        "        self.classifier = nn.Linear(\n",
        "            self.model.config.hidden_size,\n",
        "            num_labels\n",
        "        ).to(self.device)\n",
        "\n",
        "        if quantize_config:\n",
        "            self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "        # Add LoRA adapters\n",
        "        peft_config = LoraConfig(\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.05,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "        self.model = get_peft_model(self.model, peft_config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Move inputs to model's device\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        attention_mask = attention_mask.to(self.device)\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        hidden_states = outputs.hidden_states[-1]\n",
        "        pooled = hidden_states[:, 0]\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "class PropagandaDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.encodings['input_ids'][idx],\n",
        "            'attention_mask': self.encodings['attention_mask'][idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_dir = \"/content/drive/My Drive/SEMEVAL/data/\"\n",
        "\n",
        "# Load data\n",
        "def load_data(file_path: str) -> list:\n",
        "    full_path = data_dir + file_path  # <-- Use data_dir to construct the full path\n",
        "    with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def prepare_datasets(config: Dict, tokenizer, train_texts, train_labels, dev_texts, dev_labels):\n",
        "    def tokenize(texts):\n",
        "        return tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=config[\"max_length\"],\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    train_encodings = tokenize(train_texts)\n",
        "    dev_encodings = tokenize(dev_texts)\n",
        "\n",
        "    return (\n",
        "        PropagandaDataset(train_encodings, train_labels),\n",
        "        PropagandaDataset(dev_encodings, dev_labels)\n",
        "    )\n",
        "\n",
        "def train_model(model_config, train_data, dev_data, all_labels):\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_config[\"name\"],\n",
        "        use_fast=True,\n",
        "        padding_side=\"left\",\n",
        "        token=os.environ.get(\"HF_TOKEN\")\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load model\n",
        "    if model_config[\"quantize\"]:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        model = MistralForMultiLabelClassification(\n",
        "            model_config[\"name\"],\n",
        "            num_labels=len(all_labels),\n",
        "            quantize_config=bnb_config\n",
        "        )\n",
        "    else:\n",
        "        model = MistralForMultiLabelClassification(\n",
        "            model_config[\"name\"],\n",
        "            num_labels=len(all_labels)\n",
        "        ).to(device)\n",
        "\n",
        "    # Prepare data\n",
        "    train_dataset, dev_dataset = prepare_datasets(\n",
        "        model_config, tokenizer, *train_data, *dev_data\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=model_config[\"batch_size\"],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    dev_loader = DataLoader(\n",
        "        dev_dataset,\n",
        "        batch_size=model_config[\"batch_size\"],\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Initialize training components\n",
        "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "    criterion = FocalLoss(alpha=1.0, gamma=2.0)\n",
        "\n",
        "    # Training loop\n",
        "    best_f1 = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move inputs to model's device\n",
        "            inputs = {\n",
        "                'input_ids': batch['input_ids'].to(model.device),\n",
        "                'attention_mask': batch['attention_mask'].to(model.device)\n",
        "            }\n",
        "            labels = batch['labels'].to(model.device)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in dev_loader:\n",
        "                inputs = {\n",
        "                    'input_ids': batch['input_ids'].to(model.device),\n",
        "                    'attention_mask': batch['attention_mask'].to(model.device)\n",
        "                }\n",
        "                labels = batch['labels'].cpu().numpy()\n",
        "\n",
        "                outputs = model(**inputs)\n",
        "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "                all_preds.append(preds)\n",
        "                all_labels.append(labels)\n",
        "\n",
        "        # Calculate metrics\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labels = np.concatenate(all_labels)\n",
        "\n",
        "        # Threshold optimization\n",
        "        best_thresh = 0.5\n",
        "        best_f1 = 0\n",
        "        thresholds = np.linspace(0.3, 0.7, 20)\n",
        "\n",
        "        for thresh in thresholds:\n",
        "            current_f1 = f1_score(labels, (preds > thresh).astype(int),\n",
        "                                average='micro', zero_division=0)\n",
        "            if current_f1 > best_f1:\n",
        "                best_f1 = current_f1\n",
        "                best_thresh = thresh\n",
        "\n",
        "        preds_binary = (preds > best_thresh).astype(int)\n",
        "        f1_micro = f1_score(labels, preds_binary, average='micro')\n",
        "        f1_macro = f1_score(labels, preds_binary, average='macro')\n",
        "        precision = precision_score(labels, preds_binary, average='micro')\n",
        "        recall = recall_score(labels, preds_binary, average='micro')\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n",
        "        print(f\"  F1-Micro: {f1_micro:.4f} | F1-Macro: {f1_macro:.4f}\")\n",
        "        print(f\"  Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "        if f1_micro > best_f1:\n",
        "            best_f1 = f1_micro\n",
        "            torch.save(model.state_dict(), f\"best_{model_config['name'].replace('/', '_')}.pth\")\n",
        "\n",
        "    return best_f1\n",
        "\n",
        "# Main execution\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "NUM_EPOCHS = 15\n",
        "MODEL_TO_TRAIN = \"mistral-7b\"\n",
        "\n",
        "# Load and prepare data\n",
        "train_data = load_data(\"training_set_task1.txt\")\n",
        "dev_data = load_data(\"dev_set_task1.txt\")\n",
        "\n",
        "train_texts = [item[\"text\"] for item in train_data]\n",
        "train_labels = [item[\"labels\"] for item in train_data]\n",
        "dev_texts = [item[\"text\"] for item in dev_data]\n",
        "dev_labels = [item[\"labels\"] for item in dev_data]\n",
        "\n",
        "all_labels = sorted({label for labels in train_labels + dev_labels for label in labels})\n",
        "mlb = MultiLabelBinarizer(classes=all_labels)\n",
        "train_labels_enc = mlb.fit_transform(train_labels)\n",
        "dev_labels_enc = mlb.transform(dev_labels)\n",
        "\n",
        "# Run training\n",
        "print(f\"\\n=== Training {MODEL_TO_TRAIN} ===\")\n",
        "score = train_model(MODEL_CONFIGS[MODEL_TO_TRAIN],\n",
        "                   (train_texts, train_labels_enc),\n",
        "                   (dev_texts, dev_labels_enc),\n",
        "                   all_labels)\n",
        "print(f\"\\nFinal {MODEL_TO_TRAIN} F1: {score:.4f}\")"
      ],
      "metadata": {
        "id": "88-LcOVk58Gc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}